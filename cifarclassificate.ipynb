{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train = []\n",
    "label_test = []\n",
    "def reading_train(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes') \n",
    "    train = dict[b'data']\n",
    "    train = train.reshape((len(train), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    for i in dict[b'labels']:\n",
    "        label_train.append(i)\n",
    "    return train\n",
    "\n",
    "def reading_test(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes') \n",
    "    train = dict[b'data']\n",
    "    train = train.reshape((len(train), 3, 32, 32)).transpose(0, 2, 3, 1)   \n",
    "    for i in dict[b'labels']:\n",
    "        label_test.append(i)\n",
    "    return train\n",
    "\n",
    "df1 = reading_train('data_batch_1')\n",
    "df2 = reading_train('data_batch_2')\n",
    "df3 = reading_train('data_batch_3')\n",
    "df4 = reading_train('data_batch_4')\n",
    "df5 = reading_train('data_batch_5')\n",
    "ataboi = np.concatenate((df1,df2,df3,df4,df5))\n",
    "dftest = reading_test('test_batch') \n",
    "ataboi.shape\n",
    "len(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented = []\n",
    "i = 0\n",
    "for item in ataboi:\n",
    "    flip_1 = np.fliplr(item)\n",
    "    flip_2 = np.flipud(item)\n",
    "    augmented.append(flip_1)\n",
    "    augmented.append(flip_2)\n",
    "    label_train.append(label_train[i])\n",
    "    label_train.append(label_train[i])\n",
    "    i = i+1\n",
    "len(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28224fd1808>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(augmented[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 32, 32, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ataboi = np.concatenate((ataboi,arr))\n",
    "len(ataboi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[158 112  49]\n",
      "   [159 111  47]\n",
      "   [165 116  51]\n",
      "   ...\n",
      "   [137  95  36]\n",
      "   [126  91  36]\n",
      "   [116  85  33]]\n",
      "\n",
      "  [[152 112  51]\n",
      "   [151 110  40]\n",
      "   [159 114  45]\n",
      "   ...\n",
      "   [136  95  31]\n",
      "   [125  91  32]\n",
      "   [119  88  34]]\n",
      "\n",
      "  [[151 110  47]\n",
      "   [151 109  33]\n",
      "   [158 111  36]\n",
      "   ...\n",
      "   [139  98  34]\n",
      "   [130  95  34]\n",
      "   [120  89  33]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 68 124 177]\n",
      "   [ 42 100 148]\n",
      "   [ 31  88 137]\n",
      "   ...\n",
      "   [ 38  97 146]\n",
      "   [ 13  64 108]\n",
      "   [ 40  85 127]]\n",
      "\n",
      "  [[ 61 116 168]\n",
      "   [ 49 102 148]\n",
      "   [ 35  85 132]\n",
      "   ...\n",
      "   [ 26  82 130]\n",
      "   [ 29  82 126]\n",
      "   [ 20  64 107]]\n",
      "\n",
      "  [[ 54 107 160]\n",
      "   [ 56 105 149]\n",
      "   [ 45  89 132]\n",
      "   ...\n",
      "   [ 24  77 124]\n",
      "   [ 34  84 129]\n",
      "   [ 21  67 110]]]\n",
      "\n",
      "\n",
      " [[[235 235 235]\n",
      "   [231 231 231]\n",
      "   [232 232 232]\n",
      "   ...\n",
      "   [233 233 233]\n",
      "   [233 233 233]\n",
      "   [232 232 232]]\n",
      "\n",
      "  [[238 238 238]\n",
      "   [235 235 235]\n",
      "   [235 235 235]\n",
      "   ...\n",
      "   [236 236 236]\n",
      "   [236 236 236]\n",
      "   [235 235 235]]\n",
      "\n",
      "  [[237 237 237]\n",
      "   [234 234 234]\n",
      "   [234 234 234]\n",
      "   ...\n",
      "   [235 235 235]\n",
      "   [235 235 235]\n",
      "   [234 234 234]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  99  89]\n",
      "   [ 43  51  37]\n",
      "   [ 19  23  11]\n",
      "   ...\n",
      "   [169 184 179]\n",
      "   [182 197 193]\n",
      "   [188 202 201]]\n",
      "\n",
      "  [[ 82  96  82]\n",
      "   [ 46  57  36]\n",
      "   [ 36  44  22]\n",
      "   ...\n",
      "   [174 189 183]\n",
      "   [185 200 196]\n",
      "   [187 202 200]]\n",
      "\n",
      "  [[ 85 101  83]\n",
      "   [ 62  75  48]\n",
      "   [ 58  67  38]\n",
      "   ...\n",
      "   [168 183 178]\n",
      "   [180 195 191]\n",
      "   [186 200 199]]]\n",
      "\n",
      "\n",
      " [[[158 190 222]\n",
      "   [158 187 218]\n",
      "   [139 166 194]\n",
      "   ...\n",
      "   [228 231 234]\n",
      "   [237 239 243]\n",
      "   [238 241 246]]\n",
      "\n",
      "  [[170 200 229]\n",
      "   [172 199 226]\n",
      "   [151 176 201]\n",
      "   ...\n",
      "   [232 232 236]\n",
      "   [246 246 250]\n",
      "   [246 247 251]]\n",
      "\n",
      "  [[174 201 225]\n",
      "   [176 200 222]\n",
      "   [157 179 199]\n",
      "   ...\n",
      "   [230 229 232]\n",
      "   [250 249 251]\n",
      "   [245 244 247]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 31  40  45]\n",
      "   [ 30  39  44]\n",
      "   [ 26  35  40]\n",
      "   ...\n",
      "   [ 37  40  46]\n",
      "   [  9  13  14]\n",
      "   [  4   7   5]]\n",
      "\n",
      "  [[ 23  34  39]\n",
      "   [ 27  38  43]\n",
      "   [ 25  36  41]\n",
      "   ...\n",
      "   [ 19  20  24]\n",
      "   [  4   6   3]\n",
      "   [  5   7   3]]\n",
      "\n",
      "  [[ 28  41  47]\n",
      "   [ 30  43  50]\n",
      "   [ 32  45  52]\n",
      "   ...\n",
      "   [  5   6   8]\n",
      "   [  4   5   3]\n",
      "   [  7   8   7]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 20  15  12]\n",
      "   [ 19  14  11]\n",
      "   [ 15  14  11]\n",
      "   ...\n",
      "   [ 10   9   7]\n",
      "   [ 12  11   9]\n",
      "   [ 13  12  10]]\n",
      "\n",
      "  [[ 21  16  13]\n",
      "   [ 20  16  13]\n",
      "   [ 18  17  12]\n",
      "   ...\n",
      "   [ 10   9   7]\n",
      "   [ 10   9   7]\n",
      "   [ 12  11   9]]\n",
      "\n",
      "  [[ 21  16  13]\n",
      "   [ 21  17  12]\n",
      "   [ 20  18  11]\n",
      "   ...\n",
      "   [ 12  11   9]\n",
      "   [ 12  11   9]\n",
      "   [ 13  12  10]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 33  25  13]\n",
      "   [ 34  26  15]\n",
      "   [ 34  26  15]\n",
      "   ...\n",
      "   [ 28  25  52]\n",
      "   [ 29  25  58]\n",
      "   [ 23  20  42]]\n",
      "\n",
      "  [[ 33  25  14]\n",
      "   [ 34  26  15]\n",
      "   [ 34  26  15]\n",
      "   ...\n",
      "   [ 27  24  52]\n",
      "   [ 27  24  56]\n",
      "   [ 25  22  47]]\n",
      "\n",
      "  [[ 31  23  12]\n",
      "   [ 32  24  13]\n",
      "   [ 33  25  14]\n",
      "   ...\n",
      "   [ 24  23  50]\n",
      "   [ 26  23  53]\n",
      "   [ 25  20  47]]]\n",
      "\n",
      "\n",
      " [[[ 25  40  12]\n",
      "   [ 15  36   3]\n",
      "   [ 23  41  18]\n",
      "   ...\n",
      "   [ 61  82  78]\n",
      "   [ 92 113 112]\n",
      "   [ 75  89  92]]\n",
      "\n",
      "  [[ 12  25   6]\n",
      "   [ 20  37   7]\n",
      "   [ 24  36  15]\n",
      "   ...\n",
      "   [115 134 138]\n",
      "   [149 168 177]\n",
      "   [104 117 131]]\n",
      "\n",
      "  [[ 12  25  11]\n",
      "   [ 15  29   6]\n",
      "   [ 34  40  24]\n",
      "   ...\n",
      "   [154 172 182]\n",
      "   [157 175 192]\n",
      "   [116 129 151]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[100 129  81]\n",
      "   [103 132  84]\n",
      "   [104 134  86]\n",
      "   ...\n",
      "   [ 97 128  84]\n",
      "   [ 98 126  84]\n",
      "   [ 91 121  79]]\n",
      "\n",
      "  [[103 132  83]\n",
      "   [104 131  83]\n",
      "   [107 135  87]\n",
      "   ...\n",
      "   [101 132  87]\n",
      "   [ 99 127  84]\n",
      "   [ 92 121  79]]\n",
      "\n",
      "  [[ 95 126  78]\n",
      "   [ 95 123  76]\n",
      "   [101 128  81]\n",
      "   ...\n",
      "   [ 93 124  80]\n",
      "   [ 95 123  81]\n",
      "   [ 92 120  80]]]\n",
      "\n",
      "\n",
      " [[[ 73  78  75]\n",
      "   [ 98 103 113]\n",
      "   [ 99 106 114]\n",
      "   ...\n",
      "   [135 150 152]\n",
      "   [135 149 154]\n",
      "   [203 215 223]]\n",
      "\n",
      "  [[ 69  73  70]\n",
      "   [ 84  89  97]\n",
      "   [ 68  75  81]\n",
      "   ...\n",
      "   [ 85  95  89]\n",
      "   [ 71  82  80]\n",
      "   [120 133 135]]\n",
      "\n",
      "  [[ 69  73  70]\n",
      "   [ 90  95 100]\n",
      "   [ 62  71  74]\n",
      "   ...\n",
      "   [ 74  81  70]\n",
      "   [ 53  62  54]\n",
      "   [ 62  74  69]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[123 128  96]\n",
      "   [132 132 102]\n",
      "   [129 128 100]\n",
      "   ...\n",
      "   [108 107  88]\n",
      "   [ 62  60  55]\n",
      "   [ 27  27  28]]\n",
      "\n",
      "  [[115 121  91]\n",
      "   [123 124  95]\n",
      "   [129 126  99]\n",
      "   ...\n",
      "   [115 116  94]\n",
      "   [ 66  65  59]\n",
      "   [ 27  27  27]]\n",
      "\n",
      "  [[116 120  90]\n",
      "   [121 122  94]\n",
      "   [129 128 101]\n",
      "   ...\n",
      "   [116 115  94]\n",
      "   [ 68  65  58]\n",
      "   [ 27  26  26]]]]\n"
     ]
    }
   ],
   "source": [
    "print(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ataboi,dftest = ataboi / 255.0, dftest / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.61960784 0.43921569 0.19215686]\n",
      "   [0.62352941 0.43529412 0.18431373]\n",
      "   [0.64705882 0.45490196 0.2       ]\n",
      "   ...\n",
      "   [0.5372549  0.37254902 0.14117647]\n",
      "   [0.49411765 0.35686275 0.14117647]\n",
      "   [0.45490196 0.33333333 0.12941176]]\n",
      "\n",
      "  [[0.59607843 0.43921569 0.2       ]\n",
      "   [0.59215686 0.43137255 0.15686275]\n",
      "   [0.62352941 0.44705882 0.17647059]\n",
      "   ...\n",
      "   [0.53333333 0.37254902 0.12156863]\n",
      "   [0.49019608 0.35686275 0.1254902 ]\n",
      "   [0.46666667 0.34509804 0.13333333]]\n",
      "\n",
      "  [[0.59215686 0.43137255 0.18431373]\n",
      "   [0.59215686 0.42745098 0.12941176]\n",
      "   [0.61960784 0.43529412 0.14117647]\n",
      "   ...\n",
      "   [0.54509804 0.38431373 0.13333333]\n",
      "   [0.50980392 0.37254902 0.13333333]\n",
      "   [0.47058824 0.34901961 0.12941176]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.26666667 0.48627451 0.69411765]\n",
      "   [0.16470588 0.39215686 0.58039216]\n",
      "   [0.12156863 0.34509804 0.5372549 ]\n",
      "   ...\n",
      "   [0.14901961 0.38039216 0.57254902]\n",
      "   [0.05098039 0.25098039 0.42352941]\n",
      "   [0.15686275 0.33333333 0.49803922]]\n",
      "\n",
      "  [[0.23921569 0.45490196 0.65882353]\n",
      "   [0.19215686 0.4        0.58039216]\n",
      "   [0.1372549  0.33333333 0.51764706]\n",
      "   ...\n",
      "   [0.10196078 0.32156863 0.50980392]\n",
      "   [0.11372549 0.32156863 0.49411765]\n",
      "   [0.07843137 0.25098039 0.41960784]]\n",
      "\n",
      "  [[0.21176471 0.41960784 0.62745098]\n",
      "   [0.21960784 0.41176471 0.58431373]\n",
      "   [0.17647059 0.34901961 0.51764706]\n",
      "   ...\n",
      "   [0.09411765 0.30196078 0.48627451]\n",
      "   [0.13333333 0.32941176 0.50588235]\n",
      "   [0.08235294 0.2627451  0.43137255]]]\n",
      "\n",
      "\n",
      " [[[0.92156863 0.92156863 0.92156863]\n",
      "   [0.90588235 0.90588235 0.90588235]\n",
      "   [0.90980392 0.90980392 0.90980392]\n",
      "   ...\n",
      "   [0.91372549 0.91372549 0.91372549]\n",
      "   [0.91372549 0.91372549 0.91372549]\n",
      "   [0.90980392 0.90980392 0.90980392]]\n",
      "\n",
      "  [[0.93333333 0.93333333 0.93333333]\n",
      "   [0.92156863 0.92156863 0.92156863]\n",
      "   [0.92156863 0.92156863 0.92156863]\n",
      "   ...\n",
      "   [0.9254902  0.9254902  0.9254902 ]\n",
      "   [0.9254902  0.9254902  0.9254902 ]\n",
      "   [0.92156863 0.92156863 0.92156863]]\n",
      "\n",
      "  [[0.92941176 0.92941176 0.92941176]\n",
      "   [0.91764706 0.91764706 0.91764706]\n",
      "   [0.91764706 0.91764706 0.91764706]\n",
      "   ...\n",
      "   [0.92156863 0.92156863 0.92156863]\n",
      "   [0.92156863 0.92156863 0.92156863]\n",
      "   [0.91764706 0.91764706 0.91764706]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34117647 0.38823529 0.34901961]\n",
      "   [0.16862745 0.2        0.14509804]\n",
      "   [0.0745098  0.09019608 0.04313725]\n",
      "   ...\n",
      "   [0.6627451  0.72156863 0.70196078]\n",
      "   [0.71372549 0.77254902 0.75686275]\n",
      "   [0.7372549  0.79215686 0.78823529]]\n",
      "\n",
      "  [[0.32156863 0.37647059 0.32156863]\n",
      "   [0.18039216 0.22352941 0.14117647]\n",
      "   [0.14117647 0.17254902 0.08627451]\n",
      "   ...\n",
      "   [0.68235294 0.74117647 0.71764706]\n",
      "   [0.7254902  0.78431373 0.76862745]\n",
      "   [0.73333333 0.79215686 0.78431373]]\n",
      "\n",
      "  [[0.33333333 0.39607843 0.3254902 ]\n",
      "   [0.24313725 0.29411765 0.18823529]\n",
      "   [0.22745098 0.2627451  0.14901961]\n",
      "   ...\n",
      "   [0.65882353 0.71764706 0.69803922]\n",
      "   [0.70588235 0.76470588 0.74901961]\n",
      "   [0.72941176 0.78431373 0.78039216]]]\n",
      "\n",
      "\n",
      " [[[0.61960784 0.74509804 0.87058824]\n",
      "   [0.61960784 0.73333333 0.85490196]\n",
      "   [0.54509804 0.65098039 0.76078431]\n",
      "   ...\n",
      "   [0.89411765 0.90588235 0.91764706]\n",
      "   [0.92941176 0.9372549  0.95294118]\n",
      "   [0.93333333 0.94509804 0.96470588]]\n",
      "\n",
      "  [[0.66666667 0.78431373 0.89803922]\n",
      "   [0.6745098  0.78039216 0.88627451]\n",
      "   [0.59215686 0.69019608 0.78823529]\n",
      "   ...\n",
      "   [0.90980392 0.90980392 0.9254902 ]\n",
      "   [0.96470588 0.96470588 0.98039216]\n",
      "   [0.96470588 0.96862745 0.98431373]]\n",
      "\n",
      "  [[0.68235294 0.78823529 0.88235294]\n",
      "   [0.69019608 0.78431373 0.87058824]\n",
      "   [0.61568627 0.70196078 0.78039216]\n",
      "   ...\n",
      "   [0.90196078 0.89803922 0.90980392]\n",
      "   [0.98039216 0.97647059 0.98431373]\n",
      "   [0.96078431 0.95686275 0.96862745]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.12156863 0.15686275 0.17647059]\n",
      "   [0.11764706 0.15294118 0.17254902]\n",
      "   [0.10196078 0.1372549  0.15686275]\n",
      "   ...\n",
      "   [0.14509804 0.15686275 0.18039216]\n",
      "   [0.03529412 0.05098039 0.05490196]\n",
      "   [0.01568627 0.02745098 0.01960784]]\n",
      "\n",
      "  [[0.09019608 0.13333333 0.15294118]\n",
      "   [0.10588235 0.14901961 0.16862745]\n",
      "   [0.09803922 0.14117647 0.16078431]\n",
      "   ...\n",
      "   [0.0745098  0.07843137 0.09411765]\n",
      "   [0.01568627 0.02352941 0.01176471]\n",
      "   [0.01960784 0.02745098 0.01176471]]\n",
      "\n",
      "  [[0.10980392 0.16078431 0.18431373]\n",
      "   [0.11764706 0.16862745 0.19607843]\n",
      "   [0.1254902  0.17647059 0.20392157]\n",
      "   ...\n",
      "   [0.01960784 0.02352941 0.03137255]\n",
      "   [0.01568627 0.01960784 0.01176471]\n",
      "   [0.02745098 0.03137255 0.02745098]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.07843137 0.05882353 0.04705882]\n",
      "   [0.0745098  0.05490196 0.04313725]\n",
      "   [0.05882353 0.05490196 0.04313725]\n",
      "   ...\n",
      "   [0.03921569 0.03529412 0.02745098]\n",
      "   [0.04705882 0.04313725 0.03529412]\n",
      "   [0.05098039 0.04705882 0.03921569]]\n",
      "\n",
      "  [[0.08235294 0.0627451  0.05098039]\n",
      "   [0.07843137 0.0627451  0.05098039]\n",
      "   [0.07058824 0.06666667 0.04705882]\n",
      "   ...\n",
      "   [0.03921569 0.03529412 0.02745098]\n",
      "   [0.03921569 0.03529412 0.02745098]\n",
      "   [0.04705882 0.04313725 0.03529412]]\n",
      "\n",
      "  [[0.08235294 0.0627451  0.05098039]\n",
      "   [0.08235294 0.06666667 0.04705882]\n",
      "   [0.07843137 0.07058824 0.04313725]\n",
      "   ...\n",
      "   [0.04705882 0.04313725 0.03529412]\n",
      "   [0.04705882 0.04313725 0.03529412]\n",
      "   [0.05098039 0.04705882 0.03921569]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.12941176 0.09803922 0.05098039]\n",
      "   [0.13333333 0.10196078 0.05882353]\n",
      "   [0.13333333 0.10196078 0.05882353]\n",
      "   ...\n",
      "   [0.10980392 0.09803922 0.20392157]\n",
      "   [0.11372549 0.09803922 0.22745098]\n",
      "   [0.09019608 0.07843137 0.16470588]]\n",
      "\n",
      "  [[0.12941176 0.09803922 0.05490196]\n",
      "   [0.13333333 0.10196078 0.05882353]\n",
      "   [0.13333333 0.10196078 0.05882353]\n",
      "   ...\n",
      "   [0.10588235 0.09411765 0.20392157]\n",
      "   [0.10588235 0.09411765 0.21960784]\n",
      "   [0.09803922 0.08627451 0.18431373]]\n",
      "\n",
      "  [[0.12156863 0.09019608 0.04705882]\n",
      "   [0.1254902  0.09411765 0.05098039]\n",
      "   [0.12941176 0.09803922 0.05490196]\n",
      "   ...\n",
      "   [0.09411765 0.09019608 0.19607843]\n",
      "   [0.10196078 0.09019608 0.20784314]\n",
      "   [0.09803922 0.07843137 0.18431373]]]\n",
      "\n",
      "\n",
      " [[[0.09803922 0.15686275 0.04705882]\n",
      "   [0.05882353 0.14117647 0.01176471]\n",
      "   [0.09019608 0.16078431 0.07058824]\n",
      "   ...\n",
      "   [0.23921569 0.32156863 0.30588235]\n",
      "   [0.36078431 0.44313725 0.43921569]\n",
      "   [0.29411765 0.34901961 0.36078431]]\n",
      "\n",
      "  [[0.04705882 0.09803922 0.02352941]\n",
      "   [0.07843137 0.14509804 0.02745098]\n",
      "   [0.09411765 0.14117647 0.05882353]\n",
      "   ...\n",
      "   [0.45098039 0.5254902  0.54117647]\n",
      "   [0.58431373 0.65882353 0.69411765]\n",
      "   [0.40784314 0.45882353 0.51372549]]\n",
      "\n",
      "  [[0.04705882 0.09803922 0.04313725]\n",
      "   [0.05882353 0.11372549 0.02352941]\n",
      "   [0.13333333 0.15686275 0.09411765]\n",
      "   ...\n",
      "   [0.60392157 0.6745098  0.71372549]\n",
      "   [0.61568627 0.68627451 0.75294118]\n",
      "   [0.45490196 0.50588235 0.59215686]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.39215686 0.50588235 0.31764706]\n",
      "   [0.40392157 0.51764706 0.32941176]\n",
      "   [0.40784314 0.5254902  0.3372549 ]\n",
      "   ...\n",
      "   [0.38039216 0.50196078 0.32941176]\n",
      "   [0.38431373 0.49411765 0.32941176]\n",
      "   [0.35686275 0.4745098  0.30980392]]\n",
      "\n",
      "  [[0.40392157 0.51764706 0.3254902 ]\n",
      "   [0.40784314 0.51372549 0.3254902 ]\n",
      "   [0.41960784 0.52941176 0.34117647]\n",
      "   ...\n",
      "   [0.39607843 0.51764706 0.34117647]\n",
      "   [0.38823529 0.49803922 0.32941176]\n",
      "   [0.36078431 0.4745098  0.30980392]]\n",
      "\n",
      "  [[0.37254902 0.49411765 0.30588235]\n",
      "   [0.37254902 0.48235294 0.29803922]\n",
      "   [0.39607843 0.50196078 0.31764706]\n",
      "   ...\n",
      "   [0.36470588 0.48627451 0.31372549]\n",
      "   [0.37254902 0.48235294 0.31764706]\n",
      "   [0.36078431 0.47058824 0.31372549]]]\n",
      "\n",
      "\n",
      " [[[0.28627451 0.30588235 0.29411765]\n",
      "   [0.38431373 0.40392157 0.44313725]\n",
      "   [0.38823529 0.41568627 0.44705882]\n",
      "   ...\n",
      "   [0.52941176 0.58823529 0.59607843]\n",
      "   [0.52941176 0.58431373 0.60392157]\n",
      "   [0.79607843 0.84313725 0.8745098 ]]\n",
      "\n",
      "  [[0.27058824 0.28627451 0.2745098 ]\n",
      "   [0.32941176 0.34901961 0.38039216]\n",
      "   [0.26666667 0.29411765 0.31764706]\n",
      "   ...\n",
      "   [0.33333333 0.37254902 0.34901961]\n",
      "   [0.27843137 0.32156863 0.31372549]\n",
      "   [0.47058824 0.52156863 0.52941176]]\n",
      "\n",
      "  [[0.27058824 0.28627451 0.2745098 ]\n",
      "   [0.35294118 0.37254902 0.39215686]\n",
      "   [0.24313725 0.27843137 0.29019608]\n",
      "   ...\n",
      "   [0.29019608 0.31764706 0.2745098 ]\n",
      "   [0.20784314 0.24313725 0.21176471]\n",
      "   [0.24313725 0.29019608 0.27058824]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.48235294 0.50196078 0.37647059]\n",
      "   [0.51764706 0.51764706 0.4       ]\n",
      "   [0.50588235 0.50196078 0.39215686]\n",
      "   ...\n",
      "   [0.42352941 0.41960784 0.34509804]\n",
      "   [0.24313725 0.23529412 0.21568627]\n",
      "   [0.10588235 0.10588235 0.10980392]]\n",
      "\n",
      "  [[0.45098039 0.4745098  0.35686275]\n",
      "   [0.48235294 0.48627451 0.37254902]\n",
      "   [0.50588235 0.49411765 0.38823529]\n",
      "   ...\n",
      "   [0.45098039 0.45490196 0.36862745]\n",
      "   [0.25882353 0.25490196 0.23137255]\n",
      "   [0.10588235 0.10588235 0.10588235]]\n",
      "\n",
      "  [[0.45490196 0.47058824 0.35294118]\n",
      "   [0.4745098  0.47843137 0.36862745]\n",
      "   [0.50588235 0.50196078 0.39607843]\n",
      "   ...\n",
      "   [0.45490196 0.45098039 0.36862745]\n",
      "   [0.26666667 0.25490196 0.22745098]\n",
      "   [0.10588235 0.10196078 0.10196078]]]]\n"
     ]
    }
   ],
   "source": [
    "print(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization,Concatenate\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), input_shape=(32,32,3), kernel_regularizer=l2(0.001)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3,3), kernel_regularizer=l2(0.001)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Conv2D(64, (2, 2), kernel_regularizer=l2(0.001))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (2,2), kernel_regularizer=l2(0.001)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), kernel_regularizer=l2(0.001))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Conv2D(128, (3,3), kernel_regularizer=l2(0.001)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(128, kernel_regularizer=l2(0.001))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64, kernel_regularizer=l2(0.001))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32, kernel_regularizer=l2(0.001))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, kernel_regularizer=l2(0.001)))\n",
    "model.add(Activation('softmax'))'''\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling2D((2, 2)))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling2D((2, 2)))\n",
    "model5.add(Dropout(0.3))\n",
    "model5.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(MaxPooling2D((2, 2)))\n",
    "model5.add(Dropout(0.4))\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_69 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "135000/135000 [==============================] - 103s 761us/step - loss: 1.4501 - accuracy: 0.4843 - val_loss: 1.0352 - val_accuracy: 0.6243\n",
      "Epoch 2/50\n",
      "135000/135000 [==============================] - 119s 885us/step - loss: 0.9797 - accuracy: 0.6539 - val_loss: 0.8361 - val_accuracy: 0.7010\n",
      "Epoch 3/50\n",
      "135000/135000 [==============================] - 108s 800us/step - loss: 0.8246 - accuracy: 0.7121 - val_loss: 0.7408 - val_accuracy: 0.7377\n",
      "Epoch 4/50\n",
      "135000/135000 [==============================] - 107s 793us/step - loss: 0.7351 - accuracy: 0.7442 - val_loss: 0.6613 - val_accuracy: 0.7645\n",
      "Epoch 5/50\n",
      "135000/135000 [==============================] - 110s 814us/step - loss: 0.6768 - accuracy: 0.7648 - val_loss: 0.6150 - val_accuracy: 0.7832\n",
      "Epoch 6/50\n",
      "135000/135000 [==============================] - 108s 801us/step - loss: 0.6330 - accuracy: 0.7814 - val_loss: 0.5819 - val_accuracy: 0.7964\n",
      "Epoch 7/50\n",
      "135000/135000 [==============================] - 110s 817us/step - loss: 0.5959 - accuracy: 0.7946 - val_loss: 0.5618 - val_accuracy: 0.8044\n",
      "Epoch 8/50\n",
      "135000/135000 [==============================] - 107s 792us/step - loss: 0.5686 - accuracy: 0.8039 - val_loss: 0.5173 - val_accuracy: 0.8157\n",
      "Epoch 9/50\n",
      "135000/135000 [==============================] - 106s 783us/step - loss: 0.5448 - accuracy: 0.8126 - val_loss: 0.5031 - val_accuracy: 0.8259\n",
      "Epoch 10/50\n",
      "135000/135000 [==============================] - 108s 803us/step - loss: 0.5238 - accuracy: 0.8188 - val_loss: 0.4901 - val_accuracy: 0.8273\n",
      "Epoch 11/50\n",
      "135000/135000 [==============================] - 106s 783us/step - loss: 0.5040 - accuracy: 0.8261 - val_loss: 0.4688 - val_accuracy: 0.8371\n",
      "Epoch 12/50\n",
      "135000/135000 [==============================] - 107s 795us/step - loss: 0.4924 - accuracy: 0.8298 - val_loss: 0.4802 - val_accuracy: 0.8347\n",
      "Epoch 13/50\n",
      "135000/135000 [==============================] - 105s 775us/step - loss: 0.4749 - accuracy: 0.8359 - val_loss: 0.4718 - val_accuracy: 0.8372\n",
      "Epoch 14/50\n",
      "135000/135000 [==============================] - 107s 792us/step - loss: 0.4672 - accuracy: 0.8391 - val_loss: 0.4857 - val_accuracy: 0.8343\n",
      "Epoch 15/50\n",
      "135000/135000 [==============================] - 106s 789us/step - loss: 0.4532 - accuracy: 0.8433 - val_loss: 0.4587 - val_accuracy: 0.8427\n",
      "Epoch 16/50\n",
      "135000/135000 [==============================] - 107s 792us/step - loss: 0.4471 - accuracy: 0.8459 - val_loss: 0.4625 - val_accuracy: 0.8391\n",
      "Epoch 17/50\n",
      "135000/135000 [==============================] - 108s 798us/step - loss: 0.4350 - accuracy: 0.8491 - val_loss: 0.4442 - val_accuracy: 0.8503\n",
      "Epoch 18/50\n",
      "135000/135000 [==============================] - 121s 898us/step - loss: 0.4256 - accuracy: 0.8528 - val_loss: 0.4289 - val_accuracy: 0.8539\n",
      "Epoch 19/50\n",
      "135000/135000 [==============================] - 109s 806us/step - loss: 0.4177 - accuracy: 0.8552 - val_loss: 0.4367 - val_accuracy: 0.8483\n",
      "Epoch 20/50\n",
      "135000/135000 [==============================] - 104s 768us/step - loss: 0.4077 - accuracy: 0.8594 - val_loss: 0.4109 - val_accuracy: 0.8583\n",
      "Epoch 21/50\n",
      "135000/135000 [==============================] - 106s 788us/step - loss: 0.4033 - accuracy: 0.8605 - val_loss: 0.4236 - val_accuracy: 0.8555\n",
      "Epoch 22/50\n",
      "135000/135000 [==============================] - 104s 772us/step - loss: 0.3966 - accuracy: 0.8628 - val_loss: 0.4265 - val_accuracy: 0.8553\n",
      "Epoch 23/50\n",
      "135000/135000 [==============================] - 104s 770us/step - loss: 0.3919 - accuracy: 0.8641 - val_loss: 0.4310 - val_accuracy: 0.8557\n",
      "Epoch 24/50\n",
      "135000/135000 [==============================] - 104s 774us/step - loss: 0.3858 - accuracy: 0.8658 - val_loss: 0.4189 - val_accuracy: 0.8575\n",
      "Epoch 25/50\n",
      "135000/135000 [==============================] - 107s 792us/step - loss: 0.3805 - accuracy: 0.8694 - val_loss: 0.4353 - val_accuracy: 0.8500\n",
      "Epoch 26/50\n",
      "135000/135000 [==============================] - 105s 781us/step - loss: 0.3742 - accuracy: 0.8692 - val_loss: 0.4048 - val_accuracy: 0.8629\n",
      "Epoch 27/50\n",
      "135000/135000 [==============================] - 106s 784us/step - loss: 0.3692 - accuracy: 0.8721 - val_loss: 0.3973 - val_accuracy: 0.8619\n",
      "Epoch 28/50\n",
      "135000/135000 [==============================] - 104s 769us/step - loss: 0.3670 - accuracy: 0.8721 - val_loss: 0.4068 - val_accuracy: 0.8592\n",
      "Epoch 29/50\n",
      "103488/135000 [=====================>........] - ETA: 23s - loss: 0.3631 - accuracy: 0.8735"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-8c6d3a82a297>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mataboi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model5.fit(ataboi,label_train,epochs=50,batch_size=64,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 330us/step\n",
      "Test loss: 0.38915961905121804\n",
      "Test accuracy: 0.8748999834060669\n"
     ]
    }
   ],
   "source": [
    "scores = model5.evaluate(dftest, label_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8276314e-05, 2.9366276e-09, 2.9694158e-19, ..., 6.1651457e-16,\n",
       "        3.0886684e-05, 7.9112644e-13],\n",
       "       [4.1766944e-14, 3.0670308e-12, 3.0803450e-23, ..., 8.4222784e-26,\n",
       "        1.0000000e+00, 5.3620759e-16],\n",
       "       [8.0118999e-03, 4.8041728e-01, 2.2097746e-09, ..., 7.3480527e-07,\n",
       "        5.1152253e-01, 4.6661120e-05],\n",
       "       ...,\n",
       "       [1.1035643e-29, 1.3450249e-31, 1.0424476e-09, ..., 7.2111925e-09,\n",
       "        1.3922243e-28, 1.5173318e-23],\n",
       "       [1.6651195e-01, 6.9115651e-01, 9.2320004e-04, ..., 1.6630942e-03,\n",
       "        2.3224892e-15, 6.1127258e-04],\n",
       "       [2.2111463e-22, 1.2979057e-19, 3.1360392e-14, ..., 9.9999285e-01,\n",
       "        3.4390107e-23, 4.2666176e-30]], dtype=float32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
